<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interpretable or Accurate? Why Not Both? | Breaking the Jargons</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interpretable or Accurate? Why Not Both?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Building interpretable Boosting Models with IntepretML" />
<meta property="og:description" content="Building interpretable Boosting Models with IntepretML" />
<link rel="canonical" href="https://parulnith.github.io/blog/explainable%20ai/machine%20learning%20interpretibility/2021/05/27/Interpretable-or-Accurate-Why-Not-Both.html" />
<meta property="og:url" content="https://parulnith.github.io/blog/explainable%20ai/machine%20learning%20interpretibility/2021/05/27/Interpretable-or-Accurate-Why-Not-Both.html" />
<meta property="og:site_name" content="Breaking the Jargons" />
<meta property="og:image" content="https://parulnith.github.io/blog/2021-05-27-Interpretable-or-Accurate-Why-Not-Both/0.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-27T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://parulnith.github.io/blog/explainable%20ai/machine%20learning%20interpretibility/2021/05/27/Interpretable-or-Accurate-Why-Not-Both.html","@type":"BlogPosting","headline":"Interpretable or Accurate? Why Not Both?","dateModified":"2021-05-27T00:00:00-05:00","datePublished":"2021-05-27T00:00:00-05:00","image":"https://parulnith.github.io/blog/2021-05-27-Interpretable-or-Accurate-Why-Not-Both/0.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://parulnith.github.io/blog/explainable%20ai/machine%20learning%20interpretibility/2021/05/27/Interpretable-or-Accurate-Why-Not-Both.html"},"description":"Building interpretable Boosting Models with IntepretML","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://parulnith.github.io/blog/feed.xml" title="Breaking the Jargons" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Breaking the Jargons</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interpretable or Accurate? Why Not Both?</h1><p class="page-description">Building interpretable Boosting Models with IntepretML</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-27T00:00:00-05:00" itemprop="datePublished">
        May 27, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Explainable AI">Explainable AI</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Machine learning Interpretibility">Machine learning Interpretibility</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><em>The article was originally published <a href="https://towardsdatascience.com/a-better-way-to-visualize-decision-trees-with-the-dtreeviz-library-758994cdf05e?sk=ad5fcdf665e07388a829bb5320be9a6f">here</a></em></p>

<p><img src="https://cdn-images-1.medium.com/max/2560/1*kWy3VIPUPhLGl4G2-ccTPw.png" alt="" />
<sub>Image by <a href="https://pixabay.com/users/kingrise-4297632/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3183317">Kingrise</a> from <a href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3183317">Pixabay</a></sub></p>

<p>As summed up by <a href="https://arxiv.org/abs/1706.07269">Miller</a>, interpretability refers to the degree to which a human can understand the cause of a decision. A common notion in the machine learning community is that a trade-off exists between accuracy and interpretability. This means that the learning methods that are more accurate offer less interpretability and vice versa. However, of late, there has been a lot of emphasis on creating inherently interpretable models and doing away from their black box counterparts. In fact, Cynthia Rudin argues that <a href="https://www.nature.com/articles/s42256-019-0048-x">explainable black boxes should be entirely avoided for high-stakes prediction applications</a> that deeply impact human lives. So, the question is whether a model can have higher accuracy without compromising on the interpretability front?</p>

<p>Well, EBMs precisely tries to fill this void. EBMs, which stands for <a href="https://www.youtube.com/watch?v=MREiHgHgl0k">Explainable Boosting Machine</a>, are models designed to have accuracy comparable to state-of-the-art machine learning methods like Random Forest and Boosted Trees while being highly intelligible and explainable.</p>

<p>This article will look at the idea behind EBMs and implement them for a Human Resources case study via <a href="https://arxiv.org/pdf/1909.09223.pdf">InterpretML</a>, a Unified Framework for Machine Learning Interpretability.</p>

<hr />

<h2 id="machine-learning-interpretability--a-primer">Machine learning Interpretability — A Primer</h2>

<p>Machine Learning is a powerful tool and is being increasingly used in multi-faceted ways across several industries. The AI models are increasingly used to make decisions that affect people’s lives. Therefore, it becomes imperative that the predictions are fair and not biased or discriminating.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*0r29CPjHgDAVwyMki8LSzg.png" alt="" /></p>

<p><sub>Advantages of having Machine learning Interpretability in pipeline</sub></p>

<p>Machine learning interpretability has a vital role to play in such situations. Interpretability gives you the ability not only to discover a model’s mispredictions but analyze and fix the underlying cause too. Interpretability can help you debug your model, detect overfitting and data leakage, and most importantly, inspire trust between models and humans by giving explanations.</p>

<h3 id="interpretability-approaches">Interpretability Approaches</h3>

<p>The approaches employed to explain the models’ predictions can be grouped into two major categories depending upon the type of machine learning models.</p>

<h3 id="1-glassbox-models-vs-blackbox-explanations">1. Glassbox Models vs. Blackbox explanations</h3>

<p>Algorithms that are designed to be interpretable are called Glassbox models. These include algorithms like simple decision trees, rule lists, linear models, etc. Glassbox approaches typically provide exact or lossless explainability. This means it is possible to trace and reason about how any prediction is made. The interpretation of GlassBox models is <strong>Model-specific</strong> because each method is based on some specific model’s internals. For instance, the interpretation of weights in linear models count towards <a href="https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html">model-specific explanations</a>.</p>

<p>Blackbox explainers, on the contrary, are <strong>model agnostic</strong>. They can be applied to any model, and such are <strong>post-hoc</strong> in nature since they are applied after the model has been trained. Blackbox explainers work by treating the model as a BlackBox and assume that they only have access to the model’s inputs and outputs. They are particularly useful for complex algorithms like boosted trees and deep neural nets. Blackbox explainers work by repeatedly perturbing the input and analyzing the resultant changes in the model output. The examples include <a href="https://arxiv.org/abs/1705.07874">SHAP</a>, <a href="https://arxiv.org/abs/1602.04938v3">LIME</a>, <a href="https://christophm.github.io/interpretable-ml-book/pdp.html">Partial Dependence Plot</a>s, etc., to name a few.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*8ov3dWV39WHkx8SG6pMXWA.png" alt="" /></p>

<p><sub>Glassbox vs. Blackbox explainability approaches</sub></p>

<h3 id="2-local-vs-global-explanations">2. Local vs. Global explanations</h3>

<p>Another category could be depending upon the scope of explanations. Local explanations aim to explain individual predictions, while global explanations explain the entire model behavior.</p>

<p>Now that we have sufficient intuition into the interpretability mechanism employed by machine learning models, let’s switch gears and understand EBMs in more detail.</p>

<hr />

<h2 id="explainable-boosting-machine-ebms">Explainable Boosting Machine (EBMs)</h2>

<p><a href="https://dl.acm.org/doi/10.1145/2339530.2339556">EBMs</a> are Glassbox models designed to have accuracy comparable to state-of-the-art machine learning methods without compromising accuracy and explainability</p>

<p>EBM is a type of <a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">generalized additive mode</a>l or GAM for short. Linear models assume a linear relationship between the response and predictors. Thus, they are unable to capture the non-linearities in the data.</p>

<p><code class="language-plaintext highlighter-rouge">Linear Model: y = β0 + β1x1 + β2x2 + … + βn xn</code></p>

<p>To overcome this shortcoming, in the late 80’s statisticians <a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">Hastie &amp; Tibshirani developed generalized additive models</a>(GAMs), which keep the additive structure, and therefore the interpretability of the linear models. Thus, the linear relationship between the response and predictor variable gets <a href="https://datascienceplus.com/generalized-additive-models/">replaced by several non-linear smooth functions</a>(f1, f2, etc.) to model and capture the non-linearities in the data. GAMs are more accurate than simple linear models, and since they do not contain any interactions between features, users can also easily interpret them.</p>

<p><code class="language-plaintext highlighter-rouge">Additive Model: y = f1(x1) + f2(x2) + … + fn(xn)</code></p>

<p>EBMs are an improvement on the GAMs utilizing techniques like gradient boosting and bagging. EBMs include pairwise interaction terms, which increases their accuracy even further.</p>

<p><code class="language-plaintext highlighter-rouge">EBMs: y = Ʃifi (xi) + Ʃijfij(xi , xj) + Ʃijk fijk (xi , xj , xk )</code></p>

<p>The following talk from Richard Caruana, the creator of EBM, goes deeper into the intuition behind the algorithm.</p>

<p><img src="https://youtu.be/MREiHgHgl0k" alt="" /></p>

<p>The vital point to note here is that even after all these improvements, EBM still preserves the interpretability of a linear model but often matches the accuracy of powerful BlackBox models, as shown below:</p>

<p><img src="https://cdn-images-1.medium.com/max/2118/1*-gnKXfPsi5FHYcPiaLK50Q.png" alt="" /></p>

<p><sub>Classification performance for models across datasets (rows, columns)|Source : <a href="https://arxiv.org/pdf/1909.09223.pdf">InterpretML: A Unified Framework for Machine Learning Interpretability</a></sub></p>

<hr />

<h2 id="case-study-predicting-employee-attrition-using-machine-learning">Case Study: Predicting Employee Attrition Using Machine Learning</h2>
<blockquote>
  <p>Here is the <a href="https://nbviewer.jupyter.org/github/parulnith/Data-Science-Articles/blob/main/Interpretable%20or%20Accurate%3F%20Why%20not%C2%A0both/Interpretable%20or%20Accurate%3F%20Why%20not%C2%A0both.ipynb">nbviewer link</a> to the code notebook in case you want to follow along.</p>
</blockquote>

<p><img src="https://cdn-images-1.medium.com/max/3840/1*289fHah3E3BX9CkKrIJegw.jpeg" alt="[](https://pixabay.com/photos/get-me-out-escape-danger-security-1605906/)" />
<sub>Andrew Martin from Pixabay</sub></p>

<p>It’s time to get our hands dirty. In this section, we’ll train an EBM model to predict employee attrition. We’ll also compare the performance of EBMs with other algorithms. Finally, we’ll try and explain the predictions that our model made with the help of a tool called InterpretML. What is interpretML? Let’s find out.</p>

<h3 id="intepretml-a-unified-framework-for-machine-learning-interpretability">IntepretML: A Unified Framework for Machine Learning Interpretability</h3>

<p>EBMs come packaged within a Machine Learning Interpretability toolkit called <a href="http://Unified Framework for Machine Learning Interpretability">InterpretML</a>. <a href="https://github.com/interpretml/interpret">I</a>t is an open-source package for training interpretable models as well as explaining black-box systems. Within InterpretML, the explainability algorithms are organized into two major sections, i.e., <strong>Glassbox models</strong> and <strong>Blackbox explanations</strong>. This means that this tool can not only explain the decisions of inherently interpretable models but also provide possible reasoning for black-box models. The following code architecture from the <a href="https://arxiv.org/pdf/1909.09223.pdf">official paper</a> sums it nicely.</p>

<p><img src="https://cdn-images-1.medium.com/max/2030/1*MxM1QHK31w16F9U0d5t7CQ.png" alt="" />
<sub>code architecture from the official paper | Source: <a href="https://arxiv.org/pdf/1909.09223.pdf">InterpretML: A Unified Framework for Machine Learning Interpretability</a></sub></p>

<p>As per the authors, InterpretML follows four fundamental design principles:</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*3KXqAPM9YmONgN9wZf0x5g.png" alt="" /></p>

<p>InterpretML’s also offers an interactive visualization dashboard. The dashboard provides valuable insights about the nature of the dataset, model performance, and model explanations.</p>

<h3 id="dataset">Dataset</h3>

<p>We’ll use the publicly available <a href="https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset">IBM HR Analytics Employee Attrition &amp; Performance</a> dataset. This dataset contains data about an employee’s age, department, gender, education level, etc., along with information on whether the employee left the company or not, denoted by the variable Attrition. “No” represents an employee that did not leave the company, and “Yes” means an employee who left the company. We will use the dataset to build a classification model to predict the employees’ probability of attrition.</p>

<p>Here’s a snapshot of the dataset features.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*lVoiOjtNnnD8QgJBiEDCgQ.png" alt="" /></p>

<p><sub>Features of the dataset</sub></p>

<p>As stated above, InterpretML supports training interpretable models (<strong>glass-box</strong>), as well as explaining existing ML pipelines (**Blackbox **), and is supported across Windows, Mac, and Linux. Currently, the following algorithms are supported in the package:</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*n4r1n6T5p0f6c3AJUtWEEg.png" alt="" /></p>

<p><sub>Algorithms supported by InterpretML</sub></p>

<p><strong>Exploring the dataset</strong></p>

<p>The first task is always to explore the dataset and understand the distributions of various columns. InterpretML provides histogram visualizations for classification problems.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hist = ClassHistogram().explain_data(X_train, y_train, name = 'Train Data')
show(hist)
</code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/max/2410/1*G67PKzjun3wszNTHnWQTHg.gif" alt="" />
<sub>Histogram visualization</sub></p>

<p><strong>Training the model</strong></p>

<p>Training an EBM is relatively easy with InterpretML. After preprocessing our dataset and splitting it into training and a test set, the following lines of code get the job done. InterpretML conforms to the familiar scikit learn API.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ebm = ExplainableBoostingClassifier(random_state=seed, n_jobs=-1,inner_bags=100,outer_bags=100)
ebm.fit(X_train, y_train)
</code></pre></div></div>

<p>Once the model is trained, we can visualize and understand the model’s behavior globally and locally.</p>

<p><strong>Global Explanations</strong></p>

<p>Global Explanations help better understand the model’s overall behavior and the general model behavior across the population.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ebm_global = ebm.explain_global(name='EBM')
show(ebm_global)
</code></pre></div></div>

<p>The first graph that we see is the <strong>Summary plot</strong> which states that the Overtime variable is the most critical feature in determining if someone will leave the company or not.</p>

<p><img src="https://cdn-images-1.medium.com/max/2346/1*j_mIItfKqWYn-wZtsccKOQ.gif" alt="" />
<sub>Viewing Global Explanations</sub></p>

<p>We can also look deeper into each feature plot on drilling down.</p>

<p><img src="https://cdn-images-1.medium.com/max/2356/1*-FhB-jPrPGb5Y4v31voP7w.png" alt="" />
<sub>Effect of age on attrition</sub></p>

<p>The score here refers to the logit since the problem is a classification one. The higher you are on the y-axis, the higher your odds of leaving the company. However, after around 35 years of age, this behavior changes, and you have more chances of staying back.</p>

<p><strong>Local Explanations</strong></p>

<p>Local Explanations help us understand the reasons behind individual predictions and why a particular prediction was made.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ebm_local = ebm.explain_local(X_test[:5], y_test[:5], name='EBM')
show(ebm_local)
</code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*giADRNtdAvltV_E_L59BWg.png" alt="" /></p>

<p><strong>Comparing the performance with other models</strong></p>

<p>It is also easy to compare the performance of different algorithms and display the results in a dashboard format.</p>

<p><img src="https://cdn-images-1.medium.com/max/2920/1*rge1s00o89Id6o6J1pTntA.jpeg" alt="" />
<sub>Comparison Dashboard
<strong>Training BlackBox models</strong></sub></p>

<p>If required, InterpretML can also train BlackBox models and provide explanations for the predictions. Here is an example of a trained Random ForestClassifier model on the same dataset and the subsequent explanation provided by LIME.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*jhruLFrrbUv020peVaLZHA.jpeg" alt="" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>This article showcased how EBMs emerge as an excellent choice for creating both interpretable and accurate models. Personally, when machine learning models are used in high-stakes decisions, interpretability should be given a higher preference over a loss of few points of accuracy. It is not only important to see if a model works, but we as machine learning practitioners should also care about how it works and whether it works without any intentional bias.</p>

<h2 id="references">References</h2>

<p>A lot of sources and papers were referenced for this article which have been linked in the article. The primary source, however, was the <a href="https://arxiv.org/pdf/1909.09223.pdf">official paper of InterpretML</a> and its <a href="https://interpret.ml/docs/ebm.html">documentation</a>.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="parulnith/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/explainable%20ai/machine%20learning%20interpretibility/2021/05/27/Interpretable-or-Accurate-Why-Not-Both.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Posts on Machine Learning &amp; Data Science</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/parulnith" title="parulnith"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/parulpandeyindia" title="parulpandeyindia"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/pandeyparul" title="pandeyparul"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
