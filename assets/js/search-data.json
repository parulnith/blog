{
  
    
        "post0": {
            "title": "Interpretable or Accurate? Why Not Both?",
            "content": "The article was originally published here . Image by Kingrise from Pixabay . As summed up by Miller, interpretability refers to the degree to which a human can understand the cause of a decision. A common notion in the machine learning community is that a trade-off exists between accuracy and interpretability. This means that the learning methods that are more accurate offer less interpretability and vice versa. However, of late, there has been a lot of emphasis on creating inherently interpretable models and doing away from their black box counterparts. In fact, Cynthia Rudin argues that explainable black boxes should be entirely avoided for high-stakes prediction applications that deeply impact human lives. So, the question is whether a model can have higher accuracy without compromising on the interpretability front? . Well, EBMs precisely tries to fill this void. EBMs, which stands for Explainable Boosting Machine, are models designed to have accuracy comparable to state-of-the-art machine learning methods like Random Forest and Boosted Trees while being highly intelligible and explainable. . This article will look at the idea behind EBMs and implement them for a Human Resources case study via InterpretML, a Unified Framework for Machine Learning Interpretability. . . Machine learning Interpretability — A Primer . Machine Learning is a powerful tool and is being increasingly used in multi-faceted ways across several industries. The AI models are increasingly used to make decisions that affect people’s lives. Therefore, it becomes imperative that the predictions are fair and not biased or discriminating. . . Advantages of having Machine learning Interpretability in pipeline . Machine learning interpretability has a vital role to play in such situations. Interpretability gives you the ability not only to discover a model’s mispredictions but analyze and fix the underlying cause too. Interpretability can help you debug your model, detect overfitting and data leakage, and most importantly, inspire trust between models and humans by giving explanations. . Interpretability Approaches . The approaches employed to explain the models’ predictions can be grouped into two major categories depending upon the type of machine learning models. . 1. Glassbox Models vs. Blackbox explanations . Algorithms that are designed to be interpretable are called Glassbox models. These include algorithms like simple decision trees, rule lists, linear models, etc. Glassbox approaches typically provide exact or lossless explainability. This means it is possible to trace and reason about how any prediction is made. The interpretation of GlassBox models is Model-specific because each method is based on some specific model’s internals. For instance, the interpretation of weights in linear models count towards model-specific explanations. . Blackbox explainers, on the contrary, are model agnostic. They can be applied to any model, and such are post-hoc in nature since they are applied after the model has been trained. Blackbox explainers work by treating the model as a BlackBox and assume that they only have access to the model’s inputs and outputs. They are particularly useful for complex algorithms like boosted trees and deep neural nets. Blackbox explainers work by repeatedly perturbing the input and analyzing the resultant changes in the model output. The examples include SHAP, LIME, Partial Dependence Plots, etc., to name a few. . . Glassbox vs. Blackbox explainability approaches . 2. Local vs. Global explanations . Another category could be depending upon the scope of explanations. Local explanations aim to explain individual predictions, while global explanations explain the entire model behavior. . Now that we have sufficient intuition into the interpretability mechanism employed by machine learning models, let’s switch gears and understand EBMs in more detail. . . Explainable Boosting Machine (EBMs) . EBMs are Glassbox models designed to have accuracy comparable to state-of-the-art machine learning methods without compromising accuracy and explainability . EBM is a type of generalized additive model or GAM for short. Linear models assume a linear relationship between the response and predictors. Thus, they are unable to capture the non-linearities in the data. . Linear Model: y = β0 + β1x1 + β2x2 + … + βn xn . To overcome this shortcoming, in the late 80’s statisticians Hastie &amp; Tibshirani developed generalized additive models(GAMs), which keep the additive structure, and therefore the interpretability of the linear models. Thus, the linear relationship between the response and predictor variable gets replaced by several non-linear smooth functions(f1, f2, etc.) to model and capture the non-linearities in the data. GAMs are more accurate than simple linear models, and since they do not contain any interactions between features, users can also easily interpret them. . Additive Model: y = f1(x1) + f2(x2) + … + fn(xn) . EBMs are an improvement on the GAMs utilizing techniques like gradient boosting and bagging. EBMs include pairwise interaction terms, which increases their accuracy even further. . EBMs: y = Ʃifi (xi) + Ʃijfij(xi , xj) + Ʃijk fijk (xi , xj , xk ) . The following talk from Richard Caruana, the creator of EBM, goes deeper into the intuition behind the algorithm. . . The vital point to note here is that even after all these improvements, EBM still preserves the interpretability of a linear model but often matches the accuracy of powerful BlackBox models, as shown below: . . Classification performance for models across datasets (rows, columns)|Source : InterpretML: A Unified Framework for Machine Learning Interpretability . . Case Study: Predicting Employee Attrition Using Machine Learning . Here is the nbviewer link to the code notebook in case you want to follow along. . Andrew Martin from Pixabay . It’s time to get our hands dirty. In this section, we’ll train an EBM model to predict employee attrition. We’ll also compare the performance of EBMs with other algorithms. Finally, we’ll try and explain the predictions that our model made with the help of a tool called InterpretML. What is interpretML? Let’s find out. . IntepretML: A Unified Framework for Machine Learning Interpretability . EBMs come packaged within a Machine Learning Interpretability toolkit called InterpretML. It is an open-source package for training interpretable models as well as explaining black-box systems. Within InterpretML, the explainability algorithms are organized into two major sections, i.e., Glassbox models and Blackbox explanations. This means that this tool can not only explain the decisions of inherently interpretable models but also provide possible reasoning for black-box models. The following code architecture from the official paper sums it nicely. . code architecture from the official paper | Source: InterpretML: A Unified Framework for Machine Learning Interpretability . As per the authors, InterpretML follows four fundamental design principles: . . InterpretML’s also offers an interactive visualization dashboard. The dashboard provides valuable insights about the nature of the dataset, model performance, and model explanations. . Dataset . We’ll use the publicly available IBM HR Analytics Employee Attrition &amp; Performance dataset. This dataset contains data about an employee’s age, department, gender, education level, etc., along with information on whether the employee left the company or not, denoted by the variable Attrition. “No” represents an employee that did not leave the company, and “Yes” means an employee who left the company. We will use the dataset to build a classification model to predict the employees’ probability of attrition. . Here’s a snapshot of the dataset features. . . Features of the dataset . As stated above, InterpretML supports training interpretable models (glass-box), as well as explaining existing ML pipelines (**Blackbox **), and is supported across Windows, Mac, and Linux. Currently, the following algorithms are supported in the package: . . Algorithms supported by InterpretML . Exploring the dataset . The first task is always to explore the dataset and understand the distributions of various columns. InterpretML provides histogram visualizations for classification problems. . hist = ClassHistogram().explain_data(X_train, y_train, name = &#39;Train Data&#39;) show(hist) . Histogram visualization . Training the model . Training an EBM is relatively easy with InterpretML. After preprocessing our dataset and splitting it into training and a test set, the following lines of code get the job done. InterpretML conforms to the familiar scikit learn API. . ebm = ExplainableBoostingClassifier(random_state=seed, n_jobs=-1,inner_bags=100,outer_bags=100) ebm.fit(X_train, y_train) . Once the model is trained, we can visualize and understand the model’s behavior globally and locally. . Global Explanations . Global Explanations help better understand the model’s overall behavior and the general model behavior across the population. . ebm_global = ebm.explain_global(name=&#39;EBM&#39;) show(ebm_global) . The first graph that we see is the Summary plot which states that the Overtime variable is the most critical feature in determining if someone will leave the company or not. . Viewing Global Explanations . We can also look deeper into each feature plot on drilling down. . Effect of age on attrition . The score here refers to the logit since the problem is a classification one. The higher you are on the y-axis, the higher your odds of leaving the company. However, after around 35 years of age, this behavior changes, and you have more chances of staying back. . Local Explanations . Local Explanations help us understand the reasons behind individual predictions and why a particular prediction was made. . ebm_local = ebm.explain_local(X_test[:5], y_test[:5], name=&#39;EBM&#39;) show(ebm_local) . . Comparing the performance with other models . It is also easy to compare the performance of different algorithms and display the results in a dashboard format. . Comparison Dashboard Training BlackBox models . If required, InterpretML can also train BlackBox models and provide explanations for the predictions. Here is an example of a trained Random ForestClassifier model on the same dataset and the subsequent explanation provided by LIME. . . Conclusion . This article showcased how EBMs emerge as an excellent choice for creating both interpretable and accurate models. Personally, when machine learning models are used in high-stakes decisions, interpretability should be given a higher preference over a loss of few points of accuracy. It is not only important to see if a model works, but we as machine learning practitioners should also care about how it works and whether it works without any intentional bias. . References . A lot of sources and papers were referenced for this article which have been linked in the article. The primary source, however, was the official paper of InterpretML and its documentation. .",
            "url": "https://parulnith.github.io/blog/explainable%20ai/2021/05/27/Interpretable-or-Accurate-Why-Not-Both.html",
            "relUrl": "/explainable%20ai/2021/05/27/Interpretable-or-Accurate-Why-Not-Both.html",
            "date": " • May 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A better way to visualize Decision Trees with the dtreeviz library",
            "content": "The article was originally published here . It is rightly said that a picture is worth a thousand words. This axiom is equally applicable for machine learning models. If one can visualize and interpret the result, it instills more confidence in the model’s predictions. Visualizing how a machine learning model works also makes it possible to explain the results to people with less or no machine learning skills. Scikit-learn library inherently comes with the plotting capability for decision trees via the sklearn.tree.export_graphviz function. However, there are some inconsistencies with the default option. This article will look at an alternative called dtreeviz that renders better looking and intuitive visualizations while offering greater interpretability options. . dtreeviz library for visualizing tree-based models . The dtreeviz is a python library for decision tree visualization and model interpretation. According to the information available on its Github repo, the library currently supports scikit-learn, XGBoost, Spark MLlib, and LightGBM trees. . Here is a visual comparison of the visualization generated from default scikit-learn and that from dtreeviz on the famous wine quality dataset. The dataset includes 178 instances and 13 numeric predictive attributes. Each data point can belong to one of the three classes named class_0, class_1, and class_2. . . Visual comparison of the visualization generated from default scikit-learn(Left) and that from dtreeviz(Right) on the famous wine quality dataset . As is evident from the pictures above, the figure on the right delivers far more information than its counterpart on the left. There are some apparent issues with the default scikit learn visualization, for instance: . It is not immediately clear as to what the different colors represent. | There are no legends for the target class. | The visualization returns the count of the samples, and it isn’t easy to visualize the distributions. | The size of every decision node is the same regardless of the number of samples. | . The dtreeviz library plugs in these loopholes to offer a clear and more comprehensible picture. Here is what the authors have to say: . The visualizations are inspired by an educational animation by R2D3; A visual introduction to machine learning. With dtreeviz, you can visualize how the feature space is split up at decision nodes, how the training samples get distributed in leaf nodes, how the tree makes predictions for a specific observation and more. These operations are critical to for understanding how classification or regression decision trees work. . We’ll see how the dtreeviz scores over the other visualization libraries through some common examples in the following sections. For the installation instructions, please refer to the official Github page. It can be installed with pip install dtreeviz butrequires graphviz to be pre-installed. . Superior visualizations by dtreeviz . Before visualizing a decision tree, it is also essential to understand how it works. A Decision Tree is a supervised learning predictive model that uses a set of binary rules to calculate a target value. It can be used both for regression as well as classification tasks. Decision trees have three main parts: . Root Node: The node that performs the first split. | Terminal Nodes/Leaf node: Nodes that predict the outcome. | Branches: arrows connecting nodes, showing the flow from question to answer. | . The algorithm of the decision tree models works by repeatedly partitioning the data into multiple sub-spaces so that the outcomes in each final sub-space are as homogeneous as possible. This approach is technically called recursive partitioning. The algorithm tries to split the data into subsets so that each subgroup is as pure or homogeneous as possible. . The above excerpt has been taken from an article I wrote on understanding decision trees. This article goes deeper into explaining how the algorithm typically makes a decision. . Understanding Decision Trees . Now let’s get back to the dtreeviz library and plot a few of them using the wine data mentioned above. . Dataset . We’ll be using the famous red wine dataset from the Wine Quality Data Set. The dataset consists of few physicochemical tests related to the red variant of the Portuguese “Vinho Verde” wine. The goal is to model wine quality based on these tests. Since this dataset can be viewed both as a classification and regression task, it is apt for our use case. We will not have to use separate datasets for demonstrating the classification and regression examples. . Here is the nbviewer link to the notebook incase you want to follow along. . Let’s look at the first few rows of the dataset: . . A glance at the dataset . The quality parameter refers to the wine quality and is a score between 0 and 10 . . Visualizations . Creating the features and target variables for ease. . features = wine.drop(‘quality’,axis=1) target = wine[‘quality’] . Regression decision tree . For the regression example, we’ll be predicting the quality of the wine. . #Regression tree on Wine data fig = plt.figure(figsize=(25,20)) regr= tree.DecisionTreeRegressor(max_depth=3) regr.fit(features, target)viz = dtreeviz(regr, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set regression&quot;, fontname=&quot;Arial&quot;, colors = {&quot;title&quot;:&quot;purple&quot;}, scale=1.5) viz . . Regression decision tree . The horizontal dashed lines indicate the target mean for the left and right buckets in decision nodes; | A vertical dashed line indicates the split point in feature space. | The black wedge highlights the split point and identifies the exact split value. | Leaf nodes indicate the target prediction (mean) with a dashed line. | . Classification decision tree . For the classification example, we’ll predict the class of wine from the given six classes. Again the target here is the quality variable. . # Classification tree on Wine datafig = plt.figure(figsize=(25,20)) clf = tree.DecisionTreeClassifier(max_depth=3)clf.fit(features, target)# pick random X observation for demo #X = wine.data[np.random.randint(0, len(wine.data)),:]viz = dtreeviz(clf, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set classification&quot;, class_names=[&#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;4&#39;, &#39;8&#39;, &#39;3&#39;], histtype=&#39;barstacked&#39;, # default scale=1.2) viz . . Classification tree on Wine data . Unlike regressors, the target is a category for the classifiers. Therefore histograms are used to illustrate feature-target space. The stacked histograms might be challenging to read when the number of classes increases. In such cases, the histogram type parameter can be changed to barfrom barstacked, which is the default. . Customizations . The dtreeviz library also offers a bunch of customizations. I’ll showcase a few of them here: . Scaling the image . The scale parameter can be used to scale the overall image. . Trees with a left to right alignment . The orientation parameter can be set to LR to display the trees from left to right rather than top-down . fig = plt.figure(figsize=(25,20)) clf = tree.DecisionTreeClassifier(max_depth=2)clf.fit(features, target)# pick random X observation for demo #X = wine.data[np.random.randint(0, len(wine.data)),:]viz = dtreeviz(clf, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set classification&quot;, class_names=[&#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;4&#39;, &#39;8&#39;, &#39;3&#39;], **orientation=&#39;LR&#39;,** scale=1.2) viz . . Trees with left to right alignment . Prediction path of a single observation . The library also helps to isolate and understand which decision path is followed by a specific test observation. This is very useful in explaining the prediction or the results to others. For instance, let’s pick out a random sample from the dataset and traverse its decision path. . fig = plt.figure(figsize=(25,20)) clf = tree.DecisionTreeClassifier(max_depth=3)clf.fit(features, target)**# pick random X observation for demo X = features.iloc[np.random.randint(0, len(features)),:].values**viz = dtreeviz(clf, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set classification&quot;, class_names=[&#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;4&#39;, &#39;8&#39;, &#39;3&#39;], scale=1.3, X=X) viz . . Prediction path of a single observation . Saving the image . The output graph can be saved in an SVG format as follows: . viz.save_svg() . Conclusion . The dtreeviz library scores above others when it comes to plotting decision trees. The additional capability of making results interpretable is an excellent add-on; You can isolate a single data point and understand the prediction at a micro-level. This helps in better understanding a model’s predictions, and it also makes it easy to communicate the findings to others. What I have touched here is just the tip of the iceberg. The Github repository and the accompanying article by the author go into more detail, and I’ll highly recommend going through them. The links are in the reference section below. . References and further reading . The official Github repository of dtreeviz. | How to visualize decision trees — A great read on decision tree visualization by creators of dtreeviz. | Understanding Decision Trees | .",
            "url": "https://parulnith.github.io/blog/machine%20learning/2021/05/18/A-better-way-to-visualize-decision-trees.html",
            "relUrl": "/machine%20learning/2021/05/18/A-better-way-to-visualize-decision-trees.html",
            "date": " • May 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Use Colab more efficiently with these hacks",
            "content": "This article was originally published here. . Colaboratory, or “Colab” for short, are hosted Jupyter Notebooks by Google, They allow you to write and execute Python code via your browser. It is effortless to spin a Colab since it is directly integrated with your Google account. Colab provides free access to GPUs and TPUs, requires zero-configuration, and makes sharing of code seamless. . Colab has an interesting history. It initially started as an internal tool for data analysis at Google. However, later it was launched publically, and since then, many people have been using this tool to accomplish their machine learning tasks. Many students and people who do not have a GPU rely on colab for the free resources to run their machine learning experiments. . This article compiles some useful tips and hacks that I use to get my work done in Colab. I have tried to list most of the sources where I read them first. Hopefully, these tricks should help you to make the most of your Colab notebooks. . 1. Using local runtimes 🖥 . Typically, Colab provides you with free GPU resources. However, If you have your own GPUs and still want to utilize the Colab UI, there is a way. You can use the Colab UI with a local runtime as follows: . . This way, you can execute code on your local hardware and access your local file system without leaving the Colab notebook. The official documentation goes deeper into the way it works. . . 2. Scratchpad 📃 . Do you end up creating multiple Colab notebooks with names like “untitled 1.ipynb” and “untitled 2.ipynb” etc.? I guess most of us are sail in the same boat in this regard. If that’s the case, then the Cloud scratchpad notebook might be for you. The Cloud scratchpad is a special notebook available at the URL — https://colab.research.google.com/notebooks/empty.ipynb that is not automatically saved to your drive account. It is great for experimentation or nontrivial work and doesn’t take space in Google drive. . . . 3. Open GitHub Jupyter Notebooks directly in Colab 📖 . Colab notebooks are designed in a way that they can easily integrate with GitHub. This means you can both load and save Colab notebooks to GitHub, directly. There is a handy way to do that, thanks to Seungjae Ryan Lee. . When you’re on a notebook on GitHub which you want to open in Colab, replace github with githubtocolab in the URL, leaving everything else untouched. This opens the same notebook in Colab. . . . 4. Get Notified of completed cell executions 🔔 . Colab can notify you of completed executions even if you switch to another tab, window, or application. You can enable it via Tools → Settings → Site → Show desktop notifications (and allow browser notifications once prompted) to check it out. . . Here is a demo of how the notification appears even if you navigate to another tab. . . Additional Tip Do you want this same functionality in your Jupyter Notebooks as well ? Well, I have you covered. You can also enable notifications in your Jupyter notebooks for cell completion. For details you can read a blog that I wrote on the same topic - Enabling notifications in your Jupyter notebooks for cell completion . . 5. Search for all notebooks in drive 🔍 . Do you want to search for a specific Colab notebook in the drive? Navigate to the Drive search box and add : . application/vnd.google.colaboratory . This will list all the Colab notebooks in your Google Drive. Additionally, you can also specify the title and ownership of a specific notebook. For instance, if I want to search for a notebook created by me, having ‘Transfer’ in its title, I would mention the following: . . . 6. Kaggle Datasets into Google Colab 🏅 . If you are on a budget and have exhausted your GPU resources quota on Kaggle, this hack might come as a respite for you. It is possible to download any dataset seamlessly from Kaggle onto your Colab infrastructure. Here is what you need to do: . Download your Kaggle API Token : | . . On clicking the. ‘Create New API Token’ tab, a kaggle.json file will be generated that contains your API token. Create a folder named Kaggle in your Google Drive and store the kaggle.json file in it. . . Mount Drive in Colab Notebook | . . Provide the config path to kaggle.json and change the current working directory . import os os.environ[‘KAGGLE_CONFIG_DIR’] = “/content/drive/My Drive/Kaggle” . %cd /content/drive/MyDrive/Kaggle . | Copy the API of the dataset to be downloaded. . | . For standard datasets, the API can be accessed as follows; . . Forbes Billionaires 2021 dataset publically available on Kaggle . For datasets linked to competitions, the API is present under the ‘Data’ tab: . . IEEE-CIS Fraud Detection competition . Finally, run the following command to download the datasets: . !kaggle datasets download -d alexanderbader/forbes-billionaires-2021-30 #or !kaggle competitions download -c ieee-fraud-detection . | . . . 7. Accessing Visual Studio Code(VS Code) on Colab 💻 . Do you want to use Colab’s infrastructure without using notebooks? Then this tip might be for you. Thanks to the community’s efforts in creating a package called ColabCode. It is now possible to run VSCode in Colab. Technically it is accomplished via Code Server — a Visual Studio Code instance running on a remote server accessible through any web browser. Detailed instructions for installing the package can be found here - https://github.com/abhi1thakur/colabcode. . Here is a quick demo of the process. . . . 8. Data Table extension 🗄 . Colab includes an extension that renders pandas’ dataframes into interactive displays that can be filtered, sorted, and explored dynamically. To enable Data table display for Pandas dataframes, type in the following in the notebook cell: . %load_ext google.colab.data_table #To disable the display %unload_ext google.colab.data_table . Here is a quick demo of the same: https://colab.research.google.com/notebooks/data_table.ipynb . . . 9. Comparing Notebooks 👀 . Colab makes it easy to compare two notebooks. Use View &gt; Diff notebooks from the Colab menu or navigate to https://colab.research.google.com/diff and paste the Colab URLs of the notebooks to be compared, in the input boxes at the top. . . . Wrap Up . These were some of the Colab tricks that I have found very useful, especially when it comes to training machine learning models on GPUs. Even though Colab notebooks can only run for at most 12 hours, nevertheless, with the hacks shared above, you should be able to make the most out of your session. .",
            "url": "https://parulnith.github.io/blog/programming/2021/05/10/Use-Colab-more-efficiently-with-these-hacks.html",
            "relUrl": "/programming/2021/05/10/Use-Colab-more-efficiently-with-these-hacks.html",
            "date": " • May 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "The curious case of Simpson’s Paradox",
            "content": "This article was originally published here . Photo by Brendan Church on Unsplash . Statistics rarely offers a single “right”way of doing anything — Charles Wheelan in Naked Statistics . In 1996, Appleton, French, and Vanderpump* conducted an experiment to study the effect of smoking on a sample of people. The study was conducted over twenty years and included 1314 English women. Contrary to the common belief, this study showed that Smokers tend to live longer than non-smokers. Even though I am not an expert on the effects of smoking on human health, this finding is disturbing. The graph below shows that smokers had a mortality rate of 23%, while for non-smokers, it was around 31%. . . The mortality rate of smokers vs. non-smokers . Now, here’s where the things get interesting. On breaking the same data by age group, we get an entirely different picture. The results show that in most age groups, smokers have a high mortality rate compared to non-smokers. . Results of the study broken down by age group . So why the confusion?🤔 . Well, the phenomenon that we just saw above is a classic case of Simpson’s paradox, which from time to makes way into a lot of data-driven analysis. In this article, we’ll look a little deeper into it and understand how to avoid fallacies like these in our analysis. . . Simpson’s Paradox: Things aren’t always as they seem . Image by Carlos Ribeiro from Pixabay . As per Wikipedia, Simpson’s paradox also called the Yule-Simpson effect, can be defined as follows:* . Simpson’s Paradox is a phenomenon in probability and statistics, in which a trend appears in several different groups of data but disappears or reverses when these groups are combined. . In other words, the same data set can appear to show opposite trends depending on how it’s grouped. This is exactly what we saw in the smokers vs. non-smokers mortality rate example. When grouped age-wise, the data shows that non-smokers tend to live longer. But when we see an overall picture, smokers tend to live longer. So what is exactly happening here? Why are there different interpretations of the same data, and what is evading our eye in the first case? Well, The culprit, in this case, is called the Lurking variable — a conditional variable **that can affect our conclusions about the relationship between two variables — smoking and mortality in our case. . . Identifying the Lurking variable 🔍 . . Lurking means to be present in a latent or barely discernible state, although still having an effect. In the same way, a lurking variable is a variable that isn’t included in the analysis but, if included, can considerably change the outcome of the analysis. . The age groups are the lurking variable in the example discussed. When the data were grouped by age, we saw that the non-smokers were significantly older on average, and thus, more likely to die during the trial period, precisely because they were living longer in general. . . Try it out for yourself. 💻 . Here is another example where the effect of Simpson’s Paradox is easily visible. We all are aware of the Palmer Penguins🐧 dataset — the drop-in replacement for the famous iris dataset. The dataset consists of details about three species of penguins, including their culmen length and depth, their flipper length, body mass, and sex. The culmen is essentially the upper ridge of a penguin’s beak, while their wings are called flippers. The dataset is available for download on Kaggle. . Nature vector created by brgfx — www.freepik.com | Attribution 1.0 Generic (CC BY 1.0) . . Importing the necessary libraries and the dataset . import pandas as pd import seaborn as sns from scipy import stats import matplotlib.pyplot as plt %matplotlib inline #plt.rcParams[&#39;figure.figsize&#39;] = 12, 10 plt.style.use(&quot;fivethirtyeight&quot;)# for pretty graphs df = pd.read_csv(&#39;[penguins_size.csv&#39;](https://raw.githubusercontent.com/parulnith/Website-articles-datasets/master/penguins_size.csv&#39;)) df.head()&#39;) df.info() . . There are few missing values in the dataset. Let’s get rid of those. . df = df.dropna() . Let’s now visualize the relationship between the culmen length of the penguins vs. their culmen depth. We’ll use seaborn’s lmplot method (where “lm” stands for “linear model”)for the same. . . Here we see a negative association between culmen length and culmen depth for the data set. The results above demonstrate that the longer the culmen or the beak, the less dense it is. We have also calculated the correlation coefficient between the two columns to view the negative association using the Pearson correlation coefficient(PCC), referred to as Pearson’s r. The PCC is a number between -1 and 1 and measures the linear correlation between two data sets. The Scipy library provides a method called pearsonr() for the same. . Drilling down at Species level . When you drill down further and group the data species-wise, the findings reverse. The ‘hue’ parameter determines which column in the data frame should be used for color encoding. . sns.lmplot(x = &#39;culmen_length_mm&#39;,y = &#39;culmen_depth_mm&#39;, data = df, hue = &#39;species&#39;) . . Voila! What we have is a classic example of Simpson’s effect. While the culmen’s length and depth were negatively associated on a group level, the species level data exhibits an opposite association. Thus the type of species is a lurking variable here. We can also see the person’s coefficient for each of the species using the code below: . . . . Here is the nbviewer link to the notebook incase you want to follow along. . . Tools to discover Simpson’s effect 🛠 . Detecting Simpson’s effect in a dataset can be tricky and requires some careful observation and analysis. However, since this issue pops up from time to time in the statistical world, few tools have been created to help us deal with it. A paper titled “Using Simpson’s Paradox to Discover Interesting Patterns in Behavioral Data.” was released in 2018, highlighting a data-driven discovery method that leverages Simpson’s paradox to uncover interesting patterns in behavioral data. The method systematically disaggregates data to identify subgroups within a population whose behavior deviates significantly from the rest of the population. It is a great read and also has the link to the code. . . Conclusion . Data comes with a lot of power and can be easily manipulated to suit our needs and objectives. There are multiple ways of aggregating and grouping data. Depending upon how it is grouped, the data may offer confounding results. It is up to us to carefully assess all the details using the statistical tools and look for lurking variables that might affect our decisions and outcomes. .",
            "url": "https://parulnith.github.io/blog/statistics/2021/05/06/The-curious-case-of-Simpson-s-Paradox.html",
            "relUrl": "/statistics/2021/05/06/The-curious-case-of-Simpson-s-Paradox.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "What it takes to become a World No 1 on Kaggle",
            "content": "This blog was originally published at H2O.ai blog . . In this series of interviews, I present the stories of established Data Scientists and Kaggle Grandmasters at H2O.ai, who share their journey, inspirations, and accomplishments. The intention behind these interviews is to motivate and encourage others who want to understand what it takes to be a Kaggle Grandmaster. . In this article, I shall be sharing my interaction with Guanshuo Xu. He is a Kaggle Competitions Grandmaster and a Data Scientist at H2O.ai. Guanshuo obtained his Ph.D. in Electrical &amp; Electronics Engineering at the New Jersey Institute of Technology, focusing on machine learning-based image forensics and steganalysis. . Guanshuo is a man of many accomplishments. His methods for real-world image tampering detection and localization won second place in the First IEEE Image Forensics Challenge. His architectural design of deep neural networks outperformed traditional feature-based methods for the first time in image steganalysis. More recently, Guanshuo also achieved the world rank #1 in the competition’s tier on Kaggle with a win in the Alaska2 Image Steganalysis and RSNA STR Pulmonary Embolism Detection competitions. . Here is also a link to Guanshuo’s interview at CTDS.show where he discusses his achievements on Kaggle. . In this interview, we shall know more about his academic background, passion for Kaggle, and his journey to the number one title. Here is an excerpt from my conversation with Guanshuo: . . You have a background in Ph.D. in Electrical Engineering. Did it somehow influence your decision to take up Machine Learning as a career? . Guanshuo: Yes, my doctoral research used machine learning techniques to solve problems like image tampering detection and hidden data detection. For example, my last Ph.D. research project was to use deep neural nets on image steganalysis. So my education and research are directly related to machine learning. Hence, machine learning was a natural choice of career for me. . . How did your tryst with Kaggle begin, and what kept you motivated throughout your grandmaster’s journey? . Guanshuo’s Kaggle Profile . Guanshuo: From the time I discovered kaggle, I have been addicted to it. Some of the motivating factors for continuous competing on Kaggle would be the combined satisfaction of winning competitions and prize money, learning new techniques, widening and deepening my understanding of machine learning, and building surprisingly effective models. . . How does it feel to be World No 1 in Competitions? Does that bring in an extra amount of pressure while competing? . . The top 5 Kagglers in the Competition’s category as on date | Source: Kaggle’s website . Guanshuo: Honestly speaking, there is a lot more pressure to maintain the number one rank than achieve it. This is because it requires “smoother” performance. Sometimes I have to participate in more competitions simultaneously than I used to participate in before. . How do you typically approach a Kaggle problem? . A glimpse of Gunashuo’s wins on Kaggle . Guanshuo: My approach varies based on the type of problem and the goal of the competition. Nowadays, what I often do is spend days or even weeks on understanding the data and the problem and thinking of a solution which includes, for instance, guessing the distribution of the private test data, proper validation scheme, detailed modeling steps, etc. Once I have a decent picture of the overall approach, I start coding and modeling. This process helps me to gain more understanding and make corrections or adjustments, if necessary, to the overall approach. . . Could you give us a sneak peek into your toolkit like a favorite programming language, IDE, Algorithms, etc . Guanshuo: As far as my toolkit is concerned, I mostly use gedit, Python, and Pytorch for deep learning. . . The Data Science domain is rapidly evolving. How do you manage to keep up with all the latest developments? . Guanshuo: I get to know about most of the new stuff and technologies through Kaggle, my colleagues, or even by mere googling. As far as new developments in machine learning are concerned, it depends on the actual needs. I tend to filter out anything not instantly helpful and maybe keep an eye on the potentially exciting stuff. Then I get back to it as and when needed. . . A word of advice for the Data Science aspirants who have just started or wish to start their Data Science journey? . . Guanshuo: It basically depends on each person’s background and interests. However, finding a suitable platform to learn and develop skills can make things much easier in general. Additionally, taking part in Kaggle competitions can prove to be an additional helpful resource. . To achieve a world no 1 rank is no mean feat, and Guanshuo’s relentless attitude and hard work deserve all the credit. A peek into his various winning solutions on Kaggle showcases his structured approach, which is an essential element to be inculcated for problem-solving. . . Read other interviews in this series: . Rohan Rao: A Data Scientist’s journey from Sudoku to Kaggle . | Shivam Bansal: The Data Scientist who rules the ‘Data Science for Good’ competitions on Kaggle. . | Meet Yauhen: The first and the only Kaggle Grandmaster from Belarus. . | Sudalai Rajkumar: How a passion for numbers turned this Mechanical Engineer into a Kaggle Grandmaster . | Gabor Fodor: The inspiring journey of the ‘Beluga’ of Kaggle World 🐋 . | Meet the Data Scientist who just cannot stop winning on Kaggle . | Learning from others is imperative to success on Kaggle says this Turkish GrandMaster . | .",
            "url": "https://parulnith.github.io/blog/kaggle/2021/05/03/What-it-takes-to-become-a-World-No-1-on-Kaggle.html",
            "relUrl": "/kaggle/2021/05/03/What-it-takes-to-become-a-World-No-1-on-Kaggle.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Get interactive plots directly with pandas",
            "content": "Hands-on Tutorials . Get Interactive plots directly with pandas. . A tutorial on creating Plotly and Bokeh plots directly with Pandas plotting syntax . . Data exploration is by far one of the most important aspects of any data analysis task. The initial probing and preliminary checks that we perform, using the vast catalog of visualization tools, give us actionable insights into the nature of data. However, the choice of visualization tool at times is more complicated than the task itself. On the one hand, we have libraries that are easier to use but are not so helpful in showing complex relationships in data. Then there are others that render interactivity but have a considerable learning curve. Fortunately, some open-source libraries have been created that try to address this pain point effectively. . In this article, we’ll look at two such libraries, namely pandas_bokeh and cufflinks. We’ll learn how to create plotly and bokeh charts with the basic pandas plotting syntax, which we all are comfortable with. Since the article’s emphasis is on the syntax rather than the types of plots, we’ll limit ourselves to the five basic charts, i.e., line charts, bar charts, histograms, scatter plots, and pie charts. We’ll create each of these charts first with pandas plotting library and then recreate them in plotly and bokeh, albeit with a twist. . Table of Contents . Importing the Dataset . | Plotting with Pandas directly . | Bokeh Backend for Pandas — plotting with Pandas-Bokeh. . | Plotly Backend for Pandas — plotting with Cufflinks . | Conclusion . | . Dataset . We’ll work with the NIFTY-50 dataset. The NIFTY 50 index is the National Stock Exchange of India’s benchmark for the Indian equity market. The dataset is openly available on Kaggle, but we’ll be using a subset of the data containing the stock value of only four sectors, i.e., bank, pharma, IT, and FMCG. . You can download the sample dataset from here. . Let’s import the necessary libraries and dataset required for the visualization purpose: . # Importing required modules import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Reading in the data nifty_data = pd.read_csv(&#39;NIFTY_data_2020.csv&#39;,parse_dates=[&quot;Date&quot;],index_col=&#39;Date&#39;) nifty_data.head() . . We can also resample/ aggregate the data by month-end. The pandas’ library has a resample() function, which resamples the time series data. . nifty_data_resample = nifty_data.resample(rule = &#39;M&#39;).mean() nifty_data_resample . . Now that we have our dataframes ready, it is time to visualize them via different plots. . Plotting with Pandas directly . Let’s begin with the most straightforward plotting technique — pandas’ plotting functions. To plot a graph using pandas, we’ll call the .plot()method on the dataframe. . Syntax: dataframe.plot() . The plot method is just a simple wrapper around matplotlib’s **plt.plot(). **We can also specify some additional parameters like the ones mentioned below: . Some of the important Parameters -- **x** : label or position, default None Only used if data is a DataFrame. **y** : label, position or list of label, positions, default None **title**: title to be used for the plot **X and y label:** Name to use for the label on the x-axis and y-axis. **figsize** : specifies the size of the figure object. **kind** : str The kind of plot to produce: - &#39;line&#39; : line plot (default) - &#39;bar&#39; : vertical bar plot - &#39;barh&#39; : horizontal bar plot - &#39;hist&#39; : histogram - &#39;box&#39; : boxplot - &#39;kde&#39; : Kernel Density Estimation plot - &#39;density&#39; : same as &#39;kde&#39; - &#39;area&#39; : area plot - &#39;pie&#39; : pie plot - &#39;scatter&#39; : scatter plot - &#39;hexbin&#39; : hexbin plot. . For a complete list of the parameters and their usage, please refer to the documentation. Let’s now look at ways to create different plots. In this article, we’ll not go into detail explaining each plot. We’ll only focus on the syntax, which is self-explanatory if you have some experience in pandas. For a detailed understanding of the pandas’ plots, the below article will be helpful: [*Pandas Plot: Deep Dive Into Plotting Directly with Pandas ** ](https://neptune.ai/blog/pandas-plot-deep-dive-into-plotting-directly-with-pandas) *In this article, we’ll look at how to explore and visualize your data with pandas, and then we’ll dive deeper into some of the advanced capabilities for visualization with pandas. . 1. Line Plot . nifty_data.plot(title=&#39;Nifty Index values in 2020&#39;, xlabel = &#39;Values&#39;, figsize=(10,6); . . 2. Scatter Plot . nifty_data.plot(kind=&#39;scatter&#39;, x=&#39;NIFTY FMCG index&#39;, y=&#39;NIFTY Bank index&#39;, title = &#39;Scatter Plot for NIFTY Index values in 2020&#39;, figsize=(10,6)); . . 3. Histograms . nifty_data[[&#39;NIFTY FMCG index&#39;,&#39;NIFTY Bank index&#39;]].plot(kind=&#39;hist&#39;,figsize=(9,6), bins=30); . . 4. Bar plots . nifty_data_resample.plot(kind=&#39;bar&#39;,figsize=(10,6)); . . 4.1 Stacked bar plots . nifty_data_resample.plot(kind=’barh’,figsize=(10,6)); . | . . 5. Pie Charts . nifty_data_resample.index=[&#39;Jan&#39;,&#39;Feb&#39;,&#39;March&#39;,&#39;Apr&#39;,&#39;May&#39;,&#39;June&#39;,&#39;July&#39;] nifty_data_resample[&#39;NIFTY Bank index&#39;].plot.pie(legend=False, figsize=(10,6),autopct=&#39;%.1f&#39;); . . These were some of the charts that can be directly created with pandas’ dataframes. However, these charts lack interactivity and capabilities like zoom and pan. Let’s now change these existing charts in syntax into their fully interactive counterparts with just a slight change in the syntax. . Bokeh Backend for Pandas — plotting with Pandas-Bokeh. . . The bokeh library clearly stands out when it comes to data visualizations. *The *Pandas-Bokeh provides a bokeh plotting backend for Pandas, GeoPandas, and Pyspark DataFrames. This backend adds a plot_bokeh() method to the DataFrames and Series. . Installation . Pandas-Bokeh can be installed from PyPI via pip or conda . pip install pandas-bokeh or conda install -c patrikhlobil pandas-bokeh . Usage . The Pandas-Bokeh library should be imported after Pandas, GeoPandas, and/or Pyspark. . import pandas as pd import pandas_bokeh . Then one needs to define the plotting output, which can be either of the two: . **pandas_bokeh.output_notebook()**: for embedding plots in Jupyter Notebooks. **pandas_bokeh.output_file(filename):** for exporting plots as HTML. . Syntax . Now, the plotting API is accessible for a Pandas DataFrame via the dataframe.plot_bokeh(). . For more details about the plotting outputs, see the reference here or the Bokeh documentation. Let’s now plot all the five kinds of plots as plotted in the above section. We’ll be using the same datasets as used above. . import pandas as pd import pandas_bokeh **pandas_bokeh.output_notebook()** . . 1. Line Plot . nifty_data.plot_bokeh(kind=&#39;line&#39;) #equivalent to nifty_data.plot_bokeh.line() . . 2. Scatter Plot . nifty_data.plot_bokeh.scatter(x=&#39;NIFTY FMCG index&#39;, y=&#39;NIFTY Bank index&#39;); . . 3. Histograms . nifty_data[[&#39;NIFTY FMCG index&#39;,&#39;NIFTY Bank index&#39;]].plot_bokeh(kind=&#39;hist&#39;, bins=30); . . 4. Bar plots . nifty_data_resample.plot_bokeh(kind=&#39;bar&#39;,figsize=(10,6)); . . 4.1 Stacked bar plots . nifty_data_resample.plot_bokeh(kind=’barh’,stacked=True); . | . . 5. Pie Charts . nifty_data_resample.index=[&#39;Jan&#39;,&#39;Feb&#39;,&#39;March&#39;,&#39;Apr&#39;,&#39;May&#39;,&#39;June&#39;,&#39;July&#39;] nifty_data_resample.plot_bokeh.pie(y =&#39;NIFTY Bank index&#39;) . . Additionally, you can also create multiple nested pie plots within the same plot: . nifty_data_resample.plot_bokeh.pie() . . This section saw how we could seamlessly create bokeh plots without any significant change in the pandas plotting syntax. Now we can have the best of both worlds without having to learn any new format. . Plotly Backend for Pandas — plotting with Cufflinks. . . Another commonly used library for data visualization is Plotly. With plotly, you can make interactive charts in Python, R, and JavaScript. As of version 4.8, Plotly came out with a Plotly Express-powered backend for Pandas plotting, which meant that one even does not need to import plotly to create plotly like visualizations. . However, the library I want to mention here is not plotly express, but an independent third-party wrapper library around Plotly called **Cufflinks. **The beauty of cufflinks is that it is more versatile, has more functionalities, and has an API similar to pandas plotting. This means you only need to add a .iplot() method to Pandas dataframes for plotting graphs. . Installation . Make sure you have plotly installed before installing cufflinks. Read this guide for instructions. . pip install cufflinks . Usage . The repository has a lot of useful examples and notebooks to get started. . import pandas as pd import cufflinks as cf from IPython.display import display,HTML #making all charts public and setting a global theme cf.set_config_file(sharing=&#39;public&#39;,theme=&#39;white&#39;,offline=True) . That is all. We can now create visualizations with the power of plotly but with the ease of pandas. The only change in the syntax is dataframe.iplot(). . 1. Line Plot . nifty_data.iplot(kind=&#39;line&#39;) . . 2. Scatter Plot . You need to mention the plotting mode for scatter trace while creating a scatterplot. The mode could be lines, markers, text, or a combination of either of them. . nifty_data.iplot(kind=&#39;scatter&#39;,x=&#39;NIFTY FMCG index&#39;, y=&#39;NIFTY Bank index&#39;,**mode=&#39;markers&#39;**); . . 3. Histograms . nifty_data[[&#39;NIFTY FMCG index&#39;,&#39;NIFTY Bank index&#39;]].iplot(kind=&#39;hist&#39;, bins=30); . . 4. Bar plots . nifty_data_resample.iplot(kind=&#39;bar&#39;); . . 4.1 Stacked bar plots . nifty_data_resample.iplot(kind=’barh’,barmode = ‘stack’); . | . . 5. Pie Charts . nifty_data_resample.index=[&#39;Jan&#39;,&#39;Feb&#39;,&#39;March&#39;,&#39;Apr&#39;,&#39;May&#39;,&#39;June&#39;,&#39;July&#39;] nifty_data_resample.reset_index().iplot(kind=&#39;pie&#39;,labels=&#39;index&#39;,values=&#39;NIFTY Bank index&#39;) . . The Cufflinks library provides an easy way to get the power of plotly within plotly. The similarity in syntax is another point of advantage. . Conclusion . The Bokeh or a Plotly plot is self-sufficient in conveying the entire information. Based on your choice and preference, you can choose both or either of them; The primary purpose is to make the visualizations more intuitive and interactive at the same time. After going through this article, you should be able to convert the static visualizations into their interactive counterparts and take your analysis a notch higher. .",
            "url": "https://parulnith.github.io/blog/2021/04/24/Get-Interactive-plots-directly-with-pandas.html",
            "relUrl": "/2021/04/24/Get-Interactive-plots-directly-with-pandas.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Reducing memory usage in pandas with smaller datatypes",
            "content": "The article was originally published here . Photo by Tolga Ulkan on Unsplash . Managing large datasets with pandas is a pretty common issue. Despite this, there are a few tricks and tips that can help us manage the memory issue with pandas to an extent. They might not offer the best solution, but the tricks can prove to be handy at times. Hence there is no harm in getting to know them. I talked about two such alternative ways of loading large datasets in pandas in one of my previous article. . These techniques are : . Chunking: subdividing datasets into smaller parts . | Using SQL and pandas to read large data files . | . This article is a sort of continuation to the above techniques. Hence, if you haven’t read the previous article, it’ll be a good idea to do so now 😃. In this article, we’ll cover ways to optimize memory use by the effective use of datatypes. But first, let’s get to know the pandas’ datatypes in detail. . Pandas datatypes . A datatype refers to the way how data is stored in the memory. To be more succinct and quoting Wikipedia here: . a data type or simply type is an attribute of data that tells the compiler or interpreter how the programmer intends to use the data. . The primary data types consist of integers, floating-point numbers, booleans, and characters. Pandas’ also follows the same discourse. Here is a quick overview of various data types supported by pandas: . . The int and float datatypes have further subtypes depending upon the number of bytes they use to represent data. Here is a complete list: . . This is a long list but let’s touch upon few critical points: . The number preceding the name of the datatype refers to the number of bits of memory required to store a value. For instance, int8 uses 8 bits or 1 byte; int16 uses 16 bits or 2 bytes and so on. . | The larger the range, the more memory it consumes. This implies that int16 uses twice the memory as int8 while int64 uses eight times the memory as int8. . | uint8, uint16 etc. refer to unsigned integers while int refers to signed integers. There is no difference in the amount of memory allocated, but as the name suggests, unsigned integers can only store positive values, i.e., 0–255, for uint8. The same applies to uint16,uint32, and uint64 respectively. . | . The datatypes are important since the way data is stored decides what can be done with it. . Seeing things in action . Now that we have a good idea about pandas’ various data types and their representations, let’s look at ways to optimize storage when using them. I’ll be using a file comprising 1.6 GB of data summarising yellow taxi trip data for March 2016. We’ll start by importing the dataset in a pandas’ dataframe using the read_csv() function: . import pandas as pd df = pd.read_csv(&#39;yellow_tripdata_2016-03.csv&#39;) . Let’s look at its first few columns: . . By default, when pandas loads any CSV file, it automatically detects the various datatypes. Now, this is a good thing, but here is the catch. If a column consists of all integers, it assigns the int64 dtype to that column by default. Similarly, if a column consists of float values, that column gets assigned float64 dtype. . df.info() . . As stated above, three datatypes have been used in this case: . int64 for integers values, . | float64 for float values and, . | object datatype for datetime and categorical values . | . Numerical data . On inspecting our dataframe, we find that the maximum value for some of the columns will never be greater than **32767. **In such cases, it is not prudent to use int64 as the datatype, and we can easily downcast it to say, int16. Let’s understand it more concretely through an example. . For the demonstration, let’s analyze the passenger count column and calculate its memory usage. we’ll use the pandas’ memory_usage() function for the purpose. . . To understand whether a smaller datatype would suffice, let’s see the maximum and minimum values of this column. . . Since the column only consists of positive values with the max being only 9, we can easily downcast the datatype to int8 without losing any information. . . That is a considerable decrease in the memory used. Let’s take another example. This time we shall analyze the pickup_longitude column, which consists of float values. . . Now, you would agree that for the longitude(and latitude) column, values up to two decimal places would be decent in conveying the information. The advantage, on the other hand, in terms of reduction in memory usage would be immense. We can change the datatype from float64 to float16 and this would cut down the memory usage by 1/4th. . . This is great. We can similarly downcast other columns by analyzing them and can save a considerable amount of memory. . Categorical data . Till now, we have looked at only the numerical columns. Is there a way to optimize categorical columns as well? Well, yes, there are ways to reduce the memory consumption of categorical columns as well. Let’s take the case of the **store_and_fwd_flag **column, and as shown in the previous section, calculate the memory required to store it. . . The dtype ‘O’ refers to the object datatype. Now, if we’re to look at the unique values in this column, we would get: . . There are only two unique values, i.e., N and Y, which stand for No and Yes, respectively. In such cases where there are a limited number of values, we can use a more compact datatype called Categorical dtype. Here is an excerpt from the documentation itself: . Categoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales. . If we were to downcast the object type to categorical dtype, the decrease in memory usage would be as follows: . . Again, a decent amount of memory reduction is achieved. . Finally, we can also specify the datatypes for different columns at the time of loading the CSV files. This could be useful for data that throws out of memory error on loading. . . Conclusion . In this article, we saw how we could optimize the memory being used by the dataset. This is especially useful if we have limited RAM and our dataset doesn’t fit in the memory. However, it will be helpful to look at some other libraries that can handle the big data issue much more efficiently. .",
            "url": "https://parulnith.github.io/blog/pandas/2021/03/15/Reducing-memory-usage-in-pandas-with-smaller-datatypes.html",
            "relUrl": "/pandas/2021/03/15/Reducing-memory-usage-in-pandas-with-smaller-datatypes.html",
            "date": " • Mar 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Parul and I work at H2O.ai .",
          "url": "https://parulnith.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parulnith.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}