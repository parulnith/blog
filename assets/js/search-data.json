{
  
    
        "post0": {
            "title": "A better way to visualize Decision Trees with the dtreeviz library",
            "content": "It is rightly said that a picture is worth a thousand words. This axiom is equally applicable for machine learning models. If one can visualize and interpret the result, it instills more confidence in the model’s predictions. Visualizing how a machine learning model works also makes it possible to explain the results to people with less or no machine learning skills. Scikit-learn library inherently comes with the plotting capability for decision trees via the sklearn.tree.export_graphviz function. However, there are some inconsistencies with the default option. This article will look at an alternative called dtreeviz that renders better looking and intuitive visualizations while offering greater interpretability options. . dtreeviz library for visualizing tree-based models . The dtreeviz is a python library for decision tree visualization and model interpretation. According to the information available on its Github repo, the library currently supports scikit-learn, XGBoost, Spark MLlib, and LightGBM trees. . Here is a visual comparison of the visualization generated from default scikit-learn and that from dtreeviz on the famous wine quality dataset. The dataset includes 178 instances and 13 numeric predictive attributes. Each data point can belong to one of the three classes named class_0, class_1, and class_2. . . Visual comparison of the visualization generated from default scikit-learn(Left) and that from dtreeviz(Right) on the famous wine quality dataset . As is evident from the pictures above, the figure on the right delivers far more information than its counterpart on the left. There are some apparent issues with the default scikit learn visualization, for instance: . It is not immediately clear as to what the different colors represent. | There are no legends for the target class. | The visualization returns the count of the samples, and it isn’t easy to visualize the distributions. | The size of every decision node is the same regardless of the number of samples. | . The dtreeviz library plugs in these loopholes to offer a clear and more comprehensible picture. Here is what the authors have to say: . The visualizations are inspired by an educational animation by R2D3; A visual introduction to machine learning. With dtreeviz, you can visualize how the feature space is split up at decision nodes, how the training samples get distributed in leaf nodes, how the tree makes predictions for a specific observation and more. These operations are critical to for understanding how classification or regression decision trees work. . We’ll see how the dtreeviz scores over the other visualization libraries through some common examples in the following sections. For the installation instructions, please refer to the official Github page. It can be installed with pip install dtreeviz butrequires graphviz to be pre-installed. . Superior visualizations by dtreeviz . Before visualizing a decision tree, it is also essential to understand how it works. A Decision Tree is a supervised learning predictive model that uses a set of binary rules to calculate a target value. It can be used both for regression as well as classification tasks. Decision trees have three main parts: . Root Node: The node that performs the first split. | Terminal Nodes/Leaf node: Nodes that predict the outcome. | Branches: arrows connecting nodes, showing the flow from question to answer. | . The algorithm of the decision tree models works by repeatedly partitioning the data into multiple sub-spaces so that the outcomes in each final sub-space are as homogeneous as possible. This approach is technically called recursive partitioning. The algorithm tries to split the data into subsets so that each subgroup is as pure or homogeneous as possible. . The above excerpt has been taken from an article I wrote on understanding decision trees. This article goes deeper into explaining how the algorithm typically makes a decision. . Understanding Decision Trees . Now let’s get back to the dtreeviz library and plot a few of them using the wine data mentioned above. . Dataset . We’ll be using the famous red wine dataset from the Wine Quality Data Set. The dataset consists of few physicochemical tests related to the red variant of the Portuguese “Vinho Verde” wine. The goal is to model wine quality based on these tests. Since this dataset can be viewed both as a classification and regression task, it is apt for our use case. We will not have to use separate datasets for demonstrating the classification and regression examples. . Here is the nbviewer link to the notebook incase you want to follow along. . Let’s look at the first few rows of the dataset: . . A glance at the dataset . The quality parameter refers to the wine quality and is a score between 0 and 10 . . Visualizations . Creating the features and target variables for ease. . features = wine.drop(‘quality’,axis=1) target = wine[‘quality’] . Regression decision tree . For the regression example, we’ll be predicting the quality of the wine. . #Regression tree on Wine data fig = plt.figure(figsize=(25,20)) regr= tree.DecisionTreeRegressor(max_depth=3) regr.fit(features, target)viz = dtreeviz(regr, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set regression&quot;, fontname=&quot;Arial&quot;, colors = {&quot;title&quot;:&quot;purple&quot;}, scale=1.5) viz . . Regression decision tree . The horizontal dashed lines indicate the target mean for the left and right buckets in decision nodes; | A vertical dashed line indicates the split point in feature space. | The black wedge highlights the split point and identifies the exact split value. | Leaf nodes indicate the target prediction (mean) with a dashed line. | . Classification decision tree . For the classification example, we’ll predict the class of wine from the given six classes. Again the target here is the quality variable. . # Classification tree on Wine datafig = plt.figure(figsize=(25,20)) clf = tree.DecisionTreeClassifier(max_depth=3)clf.fit(features, target)# pick random X observation for demo #X = wine.data[np.random.randint(0, len(wine.data)),:]viz = dtreeviz(clf, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set classification&quot;, class_names=[&#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;4&#39;, &#39;8&#39;, &#39;3&#39;], histtype=&#39;barstacked&#39;, # default scale=1.2) viz . . Classification tree on Wine data . Unlike regressors, the target is a category for the classifiers. Therefore histograms are used to illustrate feature-target space. The stacked histograms might be challenging to read when the number of classes increases. In such cases, the histogram type parameter can be changed to barfrom barstacked, which is the default. . Customizations . The dtreeviz library also offers a bunch of customizations. I’ll showcase a few of them here: . Scaling the image . The scale parameter can be used to scale the overall image. . Trees with a left to right alignment . The orientation parameter can be set to LR to display the trees from left to right rather than top-down . fig = plt.figure(figsize=(25,20)) clf = tree.DecisionTreeClassifier(max_depth=2)clf.fit(features, target)# pick random X observation for demo #X = wine.data[np.random.randint(0, len(wine.data)),:]viz = dtreeviz(clf, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set classification&quot;, class_names=[&#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;4&#39;, &#39;8&#39;, &#39;3&#39;], **orientation=&#39;LR&#39;,** scale=1.2) viz . . Trees with left to right alignment . Prediction path of a single observation . The library also helps to isolate and understand which decision path is followed by a specific test observation. This is very useful in explaining the prediction or the results to others. For instance, let’s pick out a random sample from the dataset and traverse its decision path. . fig = plt.figure(figsize=(25,20)) clf = tree.DecisionTreeClassifier(max_depth=3)clf.fit(features, target)**# pick random X observation for demo X = features.iloc[np.random.randint(0, len(features)),:].values**viz = dtreeviz(clf, features, target, target_name=&#39;wine quality&#39;, feature_names=features.columns, title=&quot;Wine data set classification&quot;, class_names=[&#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;4&#39;, &#39;8&#39;, &#39;3&#39;], scale=1.3, X=X) viz . . Prediction path of a single observation . Saving the image . The output graph can be saved in an SVG format as follows: . viz.save_svg() . Conclusion . The dtreeviz library scores above others when it comes to plotting decision trees. The additional capability of making results interpretable is an excellent add-on; You can isolate a single data point and understand the prediction at a micro-level. This helps in better understanding a model’s predictions, and it also makes it easy to communicate the findings to others. What I have touched here is just the tip of the iceberg. The Github repository and the accompanying article by the author go into more detail, and I’ll highly recommend going through them. The links are in the reference section below. . References and further reading . The official Github repository of dtreeviz. | How to visualize decision trees — A great read on decision tree visualization by creators of dtreeviz. | Understanding Decision Trees | .",
            "url": "https://parulnith.github.io/blog/machine%20learning/data%20visualization/2021/05/18/A-better-way-to-visualize-decision-trees.html",
            "relUrl": "/machine%20learning/data%20visualization/2021/05/18/A-better-way-to-visualize-decision-trees.html",
            "date": " • May 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Use Colab more efficiently with these hacks",
            "content": "This article was originally published here. . Colaboratory, or “Colab” for short, are hosted Jupyter Notebooks by Google, They allow you to write and execute Python code via your browser. It is effortless to spin a Colab since it is directly integrated with your Google account. Colab provides free access to GPUs and TPUs, requires zero-configuration, and makes sharing of code seamless. . Colab has an interesting history. It initially started as an internal tool for data analysis at Google. However, later it was launched publically, and since then, many people have been using this tool to accomplish their machine learning tasks. Many students and people who do not have a GPU rely on colab for the free resources to run their machine learning experiments. . This article compiles some useful tips and hacks that I use to get my work done in Colab. I have tried to list most of the sources where I read them first. Hopefully, these tricks should help you to make the most of your Colab notebooks. . 1. Using local runtimes 🖥 . Typically, Colab provides you with free GPU resources. However, If you have your own GPUs and still want to utilize the Colab UI, there is a way. You can use the Colab UI with a local runtime as follows: . . This way, you can execute code on your local hardware and access your local file system without leaving the Colab notebook. The official documentation goes deeper into the way it works. . . 2. Scratchpad 📃 . Do you end up creating multiple Colab notebooks with names like “untitled 1.ipynb” and “untitled 2.ipynb” etc.? I guess most of us are sail in the same boat in this regard. If that’s the case, then the Cloud scratchpad notebook might be for you. The Cloud scratchpad is a special notebook available at the URL — https://colab.research.google.com/notebooks/empty.ipynb that is not automatically saved to your drive account. It is great for experimentation or nontrivial work and doesn’t take space in Google drive. . . . 3. Open GitHub Jupyter Notebooks directly in Colab 📖 . Colab notebooks are designed in a way that they can easily integrate with GitHub. This means you can both load and save Colab notebooks to GitHub, directly. There is a handy way to do that, thanks to Seungjae Ryan Lee. . When you’re on a notebook on GitHub which you want to open in Colab, replace github with githubtocolab in the URL, leaving everything else untouched. This opens the same notebook in Colab. . . . 4. Get Notified of completed cell executions 🔔 . Colab can notify you of completed executions even if you switch to another tab, window, or application. You can enable it via Tools → Settings → Site → Show desktop notifications (and allow browser notifications once prompted) to check it out. . . Here is a demo of how the notification appears even if you navigate to another tab. . . Additional Tip Do you want this same functionality in your Jupyter Notebooks as well ? Well, I have you covered. You can also enable notifications in your Jupyter notebooks for cell completion. For details you can read a blog that I wrote on the same topic - Enabling notifications in your Jupyter notebooks for cell completion . . 5. Search for all notebooks in drive 🔍 . Do you want to search for a specific Colab notebook in the drive? Navigate to the Drive search box and add : . application/vnd.google.colaboratory . This will list all the Colab notebooks in your Google Drive. Additionally, you can also specify the title and ownership of a specific notebook. For instance, if I want to search for a notebook created by me, having ‘Transfer’ in its title, I would mention the following: . . . 6. Kaggle Datasets into Google Colab 🏅 . If you are on a budget and have exhausted your GPU resources quota on Kaggle, this hack might come as a respite for you. It is possible to download any dataset seamlessly from Kaggle onto your Colab infrastructure. Here is what you need to do: . Download your Kaggle API Token : | . . On clicking the. ‘Create New API Token’ tab, a kaggle.json file will be generated that contains your API token. Create a folder named Kaggle in your Google Drive and store the kaggle.json file in it. . . Mount Drive in Colab Notebook | . . Provide the config path to kaggle.json and change the current working directory . import os os.environ[‘KAGGLE_CONFIG_DIR’] = “/content/drive/My Drive/Kaggle” . %cd /content/drive/MyDrive/Kaggle . | Copy the API of the dataset to be downloaded. . | . For standard datasets, the API can be accessed as follows; . . Forbes Billionaires 2021 dataset publically available on Kaggle . For datasets linked to competitions, the API is present under the ‘Data’ tab: . . IEEE-CIS Fraud Detection competition . Finally, run the following command to download the datasets: . !kaggle datasets download -d alexanderbader/forbes-billionaires-2021-30 #or !kaggle competitions download -c ieee-fraud-detection . | . . . 7. Accessing Visual Studio Code(VS Code) on Colab 💻 . Do you want to use Colab’s infrastructure without using notebooks? Then this tip might be for you. Thanks to the community’s efforts in creating a package called ColabCode. It is now possible to run VSCode in Colab. Technically it is accomplished via Code Server — a Visual Studio Code instance running on a remote server accessible through any web browser. Detailed instructions for installing the package can be found here - https://github.com/abhi1thakur/colabcode. . Here is a quick demo of the process. . . . 8. Data Table extension 🗄 . Colab includes an extension that renders pandas’ dataframes into interactive displays that can be filtered, sorted, and explored dynamically. To enable Data table display for Pandas dataframes, type in the following in the notebook cell: . %load_ext google.colab.data_table #To disable the display %unload_ext google.colab.data_table . Here is a quick demo of the same: https://colab.research.google.com/notebooks/data_table.ipynb . . . 9. Comparing Notebooks 👀 . Colab makes it easy to compare two notebooks. Use View &gt; Diff notebooks from the Colab menu or navigate to https://colab.research.google.com/diff and paste the Colab URLs of the notebooks to be compared, in the input boxes at the top. . . . Wrap Up . These were some of the Colab tricks that I have found very useful, especially when it comes to training machine learning models on GPUs. Even though Colab notebooks can only run for at most 12 hours, nevertheless, with the hacks shared above, you should be able to make the most out of your session. .",
            "url": "https://parulnith.github.io/blog/programming/colaboratory/jupyter%20notebooks/2021/05/10/Use-Colab-more-efficiently-with-these-hacks.html",
            "relUrl": "/programming/colaboratory/jupyter%20notebooks/2021/05/10/Use-Colab-more-efficiently-with-these-hacks.html",
            "date": " • May 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The curious case of Simpson’s Paradox",
            "content": "This article was originally published here . Photo by Brendan Church on Unsplash . Statistics rarely offers a single “right”way of doing anything — Charles Wheelan in Naked Statistics . In 1996, Appleton, French, and Vanderpump* conducted an experiment to study the effect of smoking on a sample of people. The study was conducted over twenty years and included 1314 English women. Contrary to the common belief, this study showed that Smokers tend to live longer than non-smokers. Even though I am not an expert on the effects of smoking on human health, this finding is disturbing. The graph below shows that smokers had a mortality rate of 23%, while for non-smokers, it was around 31%. . . The mortality rate of smokers vs. non-smokers . Now, here’s where the things get interesting. On breaking the same data by age group, we get an entirely different picture. The results show that in most age groups, smokers have a high mortality rate compared to non-smokers. . Results of the study broken down by age group . So why the confusion?🤔 . Well, the phenomenon that we just saw above is a classic case of Simpson’s paradox, which from time to makes way into a lot of data-driven analysis. In this article, we’ll look a little deeper into it and understand how to avoid fallacies like these in our analysis. . . Simpson’s Paradox: Things aren’t always as they seem . Image by Carlos Ribeiro from Pixabay . As per Wikipedia, Simpson’s paradox also called the Yule-Simpson effect, can be defined as follows:* . Simpson’s Paradox is a phenomenon in probability and statistics, in which a trend appears in several different groups of data but disappears or reverses when these groups are combined. . In other words, the same data set can appear to show opposite trends depending on how it’s grouped. This is exactly what we saw in the smokers vs. non-smokers mortality rate example. When grouped age-wise, the data shows that non-smokers tend to live longer. But when we see an overall picture, smokers tend to live longer. So what is exactly happening here? Why are there different interpretations of the same data, and what is evading our eye in the first case? Well, The culprit, in this case, is called the Lurking variable — a conditional variable **that can affect our conclusions about the relationship between two variables — smoking and mortality in our case. . . Identifying the Lurking variable 🔍 . . Lurking means to be present in a latent or barely discernible state, although still having an effect. In the same way, a lurking variable is a variable that isn’t included in the analysis but, if included, can considerably change the outcome of the analysis. . The age groups are the lurking variable in the example discussed. When the data were grouped by age, we saw that the non-smokers were significantly older on average, and thus, more likely to die during the trial period, precisely because they were living longer in general. . . Try it out for yourself. 💻 . Here is another example where the effect of Simpson’s Paradox is easily visible. We all are aware of the Palmer Penguins🐧 dataset — the drop-in replacement for the famous iris dataset. The dataset consists of details about three species of penguins, including their culmen length and depth, their flipper length, body mass, and sex. The culmen is essentially the upper ridge of a penguin’s beak, while their wings are called flippers. The dataset is available for download on Kaggle. . Nature vector created by brgfx — www.freepik.com | Attribution 1.0 Generic (CC BY 1.0) . . Importing the necessary libraries and the dataset . import pandas as pd import seaborn as sns from scipy import stats import matplotlib.pyplot as plt %matplotlib inline #plt.rcParams[&#39;figure.figsize&#39;] = 12, 10 plt.style.use(&quot;fivethirtyeight&quot;)# for pretty graphs df = pd.read_csv(&#39;[penguins_size.csv&#39;](https://raw.githubusercontent.com/parulnith/Website-articles-datasets/master/penguins_size.csv&#39;)) df.head()&#39;) df.info() . . There are few missing values in the dataset. Let’s get rid of those. . df = df.dropna() . Let’s now visualize the relationship between the culmen length of the penguins vs. their culmen depth. We’ll use seaborn’s lmplot method (where “lm” stands for “linear model”)for the same. . . Here we see a negative association between culmen length and culmen depth for the data set. The results above demonstrate that the longer the culmen or the beak, the less dense it is. We have also calculated the correlation coefficient between the two columns to view the negative association using the Pearson correlation coefficient(PCC), referred to as Pearson’s r. The PCC is a number between -1 and 1 and measures the linear correlation between two data sets. The Scipy library provides a method called pearsonr() for the same. . Drilling down at Species level . When you drill down further and group the data species-wise, the findings reverse. The ‘hue’ parameter determines which column in the data frame should be used for color encoding. . sns.lmplot(x = &#39;culmen_length_mm&#39;,y = &#39;culmen_depth_mm&#39;, data = df, hue = &#39;species&#39;) . . Voila! What we have is a classic example of Simpson’s effect. While the culmen’s length and depth were negatively associated on a group level, the species level data exhibits an opposite association. Thus the type of species is a lurking variable here. We can also see the person’s coefficient for each of the species using the code below: . . . . Here is the nbviewer link to the notebook incase you want to follow along. . . Tools to discover Simpson’s effect 🛠 . Detecting Simpson’s effect in a dataset can be tricky and requires some careful observation and analysis. However, since this issue pops up from time to time in the statistical world, few tools have been created to help us deal with it. A paper titled “Using Simpson’s Paradox to Discover Interesting Patterns in Behavioral Data.” was released in 2018, highlighting a data-driven discovery method that leverages Simpson’s paradox to uncover interesting patterns in behavioral data. The method systematically disaggregates data to identify subgroups within a population whose behavior deviates significantly from the rest of the population. It is a great read and also has the link to the code. . . Conclusion . Data comes with a lot of power and can be easily manipulated to suit our needs and objectives. There are multiple ways of aggregating and grouping data. Depending upon how it is grouped, the data may offer confounding results. It is up to us to carefully assess all the details using the statistical tools and look for lurking variables that might affect our decisions and outcomes. .",
            "url": "https://parulnith.github.io/blog/statistics/2021/05/06/The-curious-case-of-Simpson-s-Paradox.html",
            "relUrl": "/statistics/2021/05/06/The-curious-case-of-Simpson-s-Paradox.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "What it takes to become a World No 1 on Kaggle",
            "content": "This blog was originally published at H2O.ai blog . . In this series of interviews, I present the stories of established Data Scientists and Kaggle Grandmasters at H2O.ai, who share their journey, inspirations, and accomplishments. The intention behind these interviews is to motivate and encourage others who want to understand what it takes to be a Kaggle Grandmaster. . In this article, I shall be sharing my interaction with Guanshuo Xu. He is a Kaggle Competitions Grandmaster and a Data Scientist at H2O.ai. Guanshuo obtained his Ph.D. in Electrical &amp; Electronics Engineering at the New Jersey Institute of Technology, focusing on machine learning-based image forensics and steganalysis. . Guanshuo is a man of many accomplishments. His methods for real-world image tampering detection and localization won second place in the First IEEE Image Forensics Challenge. His architectural design of deep neural networks outperformed traditional feature-based methods for the first time in image steganalysis. More recently, Guanshuo also achieved the world rank #1 in the competition’s tier on Kaggle with a win in the Alaska2 Image Steganalysis and RSNA STR Pulmonary Embolism Detection competitions. . Here is also a link to Guanshuo’s interview at CTDS.show where he discusses his achievements on Kaggle. . In this interview, we shall know more about his academic background, passion for Kaggle, and his journey to the number one title. Here is an excerpt from my conversation with Guanshuo: . . You have a background in Ph.D. in Electrical Engineering. Did it somehow influence your decision to take up Machine Learning as a career? . Guanshuo: Yes, my doctoral research used machine learning techniques to solve problems like image tampering detection and hidden data detection. For example, my last Ph.D. research project was to use deep neural nets on image steganalysis. So my education and research are directly related to machine learning. Hence, machine learning was a natural choice of career for me. . . How did your tryst with Kaggle begin, and what kept you motivated throughout your grandmaster’s journey? . Guanshuo’s Kaggle Profile . Guanshuo: From the time I discovered kaggle, I have been addicted to it. Some of the motivating factors for continuous competing on Kaggle would be the combined satisfaction of winning competitions and prize money, learning new techniques, widening and deepening my understanding of machine learning, and building surprisingly effective models. . . How does it feel to be World No 1 in Competitions? Does that bring in an extra amount of pressure while competing? . . The top 5 Kagglers in the Competition’s category as on date | Source: Kaggle’s website . Guanshuo: Honestly speaking, there is a lot more pressure to maintain the number one rank than achieve it. This is because it requires “smoother” performance. Sometimes I have to participate in more competitions simultaneously than I used to participate in before. . How do you typically approach a Kaggle problem? . A glimpse of Gunashuo’s wins on Kaggle . Guanshuo: My approach varies based on the type of problem and the goal of the competition. Nowadays, what I often do is spend days or even weeks on understanding the data and the problem and thinking of a solution which includes, for instance, guessing the distribution of the private test data, proper validation scheme, detailed modeling steps, etc. Once I have a decent picture of the overall approach, I start coding and modeling. This process helps me to gain more understanding and make corrections or adjustments, if necessary, to the overall approach. . . Could you give us a sneak peek into your toolkit like a favorite programming language, IDE, Algorithms, etc . Guanshuo: As far as my toolkit is concerned, I mostly use gedit, Python, and Pytorch for deep learning. . . The Data Science domain is rapidly evolving. How do you manage to keep up with all the latest developments? . Guanshuo: I get to know about most of the new stuff and technologies through Kaggle, my colleagues, or even by mere googling. As far as new developments in machine learning are concerned, it depends on the actual needs. I tend to filter out anything not instantly helpful and maybe keep an eye on the potentially exciting stuff. Then I get back to it as and when needed. . . A word of advice for the Data Science aspirants who have just started or wish to start their Data Science journey? . . Guanshuo: It basically depends on each person’s background and interests. However, finding a suitable platform to learn and develop skills can make things much easier in general. Additionally, taking part in Kaggle competitions can prove to be an additional helpful resource. . To achieve a world no 1 rank is no mean feat, and Guanshuo’s relentless attitude and hard work deserve all the credit. A peek into his various winning solutions on Kaggle showcases his structured approach, which is an essential element to be inculcated for problem-solving. . . Read other interviews in this series: . Rohan Rao: A Data Scientist’s journey from Sudoku to Kaggle . | Shivam Bansal: The Data Scientist who rules the ‘Data Science for Good’ competitions on Kaggle. . | Meet Yauhen: The first and the only Kaggle Grandmaster from Belarus. . | Sudalai Rajkumar: How a passion for numbers turned this Mechanical Engineer into a Kaggle Grandmaster . | Gabor Fodor: The inspiring journey of the ‘Beluga’ of Kaggle World 🐋 . | Meet the Data Scientist who just cannot stop winning on Kaggle . | Learning from others is imperative to success on Kaggle says this Turkish GrandMaster . | .",
            "url": "https://parulnith.github.io/blog/kaggle/interviews/2021/05/03/What-it-takes-to-become-a-World-No-1-on-Kaggle.html",
            "relUrl": "/kaggle/interviews/2021/05/03/What-it-takes-to-become-a-World-No-1-on-Kaggle.html",
            "date": " • May 3, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Parul and I work at H2O.ai .",
          "url": "https://parulnith.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parulnith.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}